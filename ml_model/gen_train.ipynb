{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4e90864b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import re\n",
    "from playlist import optimized_pomodoro_playlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543b2d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lowest_min_duration = 5\n",
    "highest_min_duration = 365\n",
    "increment_error_value = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "bfa65cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41.49999999999987\n",
      "3600\n",
      "Max sequence length: 17\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Token mapping\n",
    "token_map = {'W': 0, 'S': 1, 'L': 2, 'EOS': 3, 'PAD': 4}\n",
    "inv_token_map = {v: k for k, v in token_map.items()}\n",
    "\n",
    "def expand_sequence(seq):\n",
    "    # Expand patterns like '2×WSWSWL' to 'WSWSWLWSWSWL'\n",
    "    match = re.match(r\"(\\d+)×([WSL]+)\", seq)\n",
    "    if match:\n",
    "        n, pat = match.groups()\n",
    "        seq = pat * int(n)\n",
    "    seq = seq.replace('+', '')  # Remove '+'\n",
    "    return seq\n",
    "\n",
    "all_lengths = []\n",
    "\n",
    "mins_list = np.arange(lowest_min_duration, highest_min_duration, increment_error_value)\n",
    "\n",
    "print(mins_list[highest_min_duration])\n",
    "print(len(mins_list))\n",
    "\n",
    "\n",
    "for mins in mins_list:\n",
    "    result = optimized_pomodoro_playlist(f\"{int(mins)}:00\", code_format=False)\n",
    "    sequence = expand_sequence(result['sequence'])\n",
    "    sequence = re.sub(r'[^WSL]', '', sequence)\n",
    "    all_lengths.append(len(sequence))\n",
    "print(\"Max sequence length:\", max(all_lengths)+1)\n",
    "\n",
    "max_seq_len = max(all_lengths)+1  # Set according to your longest pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6c796804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def make_example_with_delta(duration_mins):\n",
    "    result = optimized_pomodoro_playlist(f\"{int(duration_mins)}:00\", code_format=False)\n",
    "    sequence = re.sub(r'[^WSL]', '', expand_sequence(result['sequence']))\n",
    "    seq_tokens = [token_map[ch] for ch in sequence] + [token_map['EOS']]  # EOS at end\n",
    "    seq_tokens += [token_map['PAD']] * (max_seq_len - len(seq_tokens))\n",
    "    \n",
    "    # Durations per token\n",
    "    durations = []\n",
    "    delta_labels = []\n",
    "    w_idx = 0\n",
    "    short_break = result['short_break']\n",
    "    long_break = result['long_break'] if result['long_break'] is not None else 0\n",
    "    for ch in sequence:\n",
    "        if ch == 'W':\n",
    "            durations.append(result['work_sessions'][w_idx])\n",
    "            delta_labels.append(0)\n",
    "            w_idx += 1\n",
    "        elif ch == 'S':\n",
    "            durations.append(short_break)\n",
    "            delta_labels.append(0)\n",
    "        elif ch == 'L':\n",
    "            # Only delta for L tokens: long_break - short_break\n",
    "            durations.append(long_break)\n",
    "            delta = max(long_break - short_break, 0)\n",
    "            delta_labels.append(delta)\n",
    "    # Pad to max_seq_len\n",
    "    durations += [0] * (max_seq_len - len(durations))\n",
    "    delta_labels += [0] * (max_seq_len - len(delta_labels))\n",
    "    return float(duration_mins) / 360.0, seq_tokens, durations, delta_labels\n",
    "\n",
    "# Generate dataset\n",
    "X = []\n",
    "Y_seq = []\n",
    "Y_dur = []\n",
    "Y_delta = []\n",
    "for mins in mins_list:\n",
    "    x, y_seq, y_dur, y_delta = make_example_with_delta(mins)\n",
    "    X.append([x])\n",
    "    Y_seq.append(y_seq)\n",
    "    Y_dur.append(y_dur)\n",
    "    Y_delta.append(y_delta)\n",
    "\n",
    "# Convert to tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "Y_seq_tensor = torch.tensor(Y_seq, dtype=torch.long)\n",
    "Y_dur_tensor = torch.tensor(Y_dur, dtype=torch.float32)\n",
    "Y_delta_tensor = torch.tensor(Y_delta, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "bd1a754a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "class PomodoroSeq2SeqDelta(nn.Module):\n",
    "    def __init__(self, input_dim=1, hidden_dim=64, vocab_size=4, max_len=16):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Linear(input_dim, hidden_dim)\n",
    "        self.decoder = nn.GRU(hidden_dim, hidden_dim, batch_first=True)\n",
    "        self.token_head = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.dur_head = nn.Linear(hidden_dim, 1)\n",
    "        self.delta_head = nn.Linear(hidden_dim, 1)\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc = torch.relu(self.encoder(x))\n",
    "        dec_input = enc.unsqueeze(1).repeat(1, self.max_len, 1)\n",
    "        output, _ = self.decoder(dec_input)\n",
    "        token_logits = self.token_head(output)           # (batch, max_len, vocab_size)\n",
    "        dur_pred = self.dur_head(output).squeeze(-1)     # (batch, max_len)\n",
    "        delta_pred = torch.relu(self.delta_head(output)).squeeze(-1)   # (batch, max_len), always >= 0\n",
    "        return token_logits, dur_pred, delta_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b69bc791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 29.71 GiB. GPU 0 has a total capacity of 6.00 GiB of which 82.00 MiB is free. Of the allocated memory 4.75 GiB is allocated by PyTorch, and 144.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[76], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 19\u001b[0m token_logits, dur_pred, delta_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m loss_tokens \u001b[38;5;241m=\u001b[39m token_criterion(token_logits\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m), Y_seq_tensor\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     21\u001b[0m loss_dur \u001b[38;5;241m=\u001b[39m dur_criterion(dur_pred, Y_dur_tensor)\n",
      "File \u001b[1;32mc:\\Users\\cagat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\cagat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[75], line 15\u001b[0m, in \u001b[0;36mPomodoroSeq2SeqDelta.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     13\u001b[0m enc \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x))\n\u001b[0;32m     14\u001b[0m dec_input \u001b[38;5;241m=\u001b[39m enc\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_len, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m output, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdec_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m token_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_head(output)           \u001b[38;5;66;03m# (batch, max_len, vocab_size)\u001b[39;00m\n\u001b[0;32m     17\u001b[0m dur_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdur_head(output)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)     \u001b[38;5;66;03m# (batch, max_len)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\cagat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\cagat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\cagat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:1393\u001b[0m, in \u001b[0;36mGRU.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m   1391\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_forward_args(\u001b[38;5;28minput\u001b[39m, hx, batch_sizes)\n\u001b[0;32m   1392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1393\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgru\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1394\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1395\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1396\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1397\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1398\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1399\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1400\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1401\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1402\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1403\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1404\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1405\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mgru(\n\u001b[0;32m   1406\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   1407\u001b[0m         batch_sizes,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1414\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional,\n\u001b[0;32m   1415\u001b[0m     )\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 29.71 GiB. GPU 0 has a total capacity of 6.00 GiB of which 82.00 MiB is free. Of the allocated memory 4.75 GiB is allocated by PyTorch, and 144.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = PomodoroSeq2SeqDelta(max_len=max_seq_len).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "token_criterion = nn.CrossEntropyLoss(ignore_index=token_map['PAD'])\n",
    "dur_criterion = nn.MSELoss()\n",
    "delta_criterion = nn.MSELoss()\n",
    "\n",
    "X_tensor = X_tensor.to(device)\n",
    "Y_seq_tensor = Y_seq_tensor.to(device)\n",
    "Y_dur_tensor = Y_dur_tensor.to(device)\n",
    "Y_delta_tensor = Y_delta_tensor.to(device)\n",
    "\n",
    "num_epochs = 10000\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    token_logits, dur_pred, delta_pred = model(X_tensor)\n",
    "    loss_tokens = token_criterion(token_logits.view(-1, 4), Y_seq_tensor.view(-1))\n",
    "    loss_dur = dur_criterion(dur_pred, Y_dur_tensor)\n",
    "    loss_delta = delta_criterion(delta_pred, Y_delta_tensor)\n",
    "    loss = loss_tokens + loss_dur + loss_delta\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (epoch + 1) % 100 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Token: {loss_tokens.item():.4f} | Dur: {loss_dur.item():.4f} | Delta: {loss_delta.item():.4f} | Total: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f05d097f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def decode_output_delta(token_seq, dur_seq, delta_seq):\n",
    "    sequence = []\n",
    "    work_sessions = []\n",
    "    short_break = None\n",
    "    long_break = None\n",
    "    w_idx = 0\n",
    "    last_short = None\n",
    "    for i, (tok, dur, delta) in enumerate(zip(token_seq, dur_seq, delta_seq)):\n",
    "        ch = inv_token_map[tok]\n",
    "        if ch in ('PAD', 'EOS'):\n",
    "            break\n",
    "        sequence.append(ch)\n",
    "        if ch == 'W':\n",
    "            work_sessions.append(int(round(dur)))\n",
    "            w_idx += 1\n",
    "        elif ch == 'S':\n",
    "            short_break = int(round(dur))\n",
    "            last_short = short_break\n",
    "        elif ch == 'L':\n",
    "            # Use delta to guarantee long_break > short_break\n",
    "            long_break = int(round((last_short if last_short is not None else 0) + delta))\n",
    "    return ''.join(sequence), work_sessions, short_break, long_break\n",
    "\n",
    "def predict_full_delta(model, duration_mins):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x = torch.tensor([[duration_mins / 360.0]], dtype=torch.float32).to(device)\n",
    "        token_logits, dur_pred, delta_pred = model(x)\n",
    "        pred_tokens = torch.argmax(token_logits, dim=2).cpu().numpy()[0]\n",
    "        pred_durs = dur_pred.cpu().numpy()[0]\n",
    "        pred_deltas = delta_pred.cpu().numpy()[0]\n",
    "        pattern, work_sessions, short_break, long_break = decode_output_delta(pred_tokens, pred_durs, pred_deltas)\n",
    "        return {\n",
    "            'duration_minutes': duration_mins,\n",
    "            'pattern': pattern,\n",
    "            'work_sessions': work_sessions,\n",
    "            'short_break': short_break,\n",
    "            'long_break': long_break\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "65ba6695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def format_minutes_to_mmss(minutes: float) -> str:\n",
    "    total_seconds = round(minutes * 60)\n",
    "    mm = total_seconds // 60\n",
    "    ss = total_seconds % 60\n",
    "    return f\"{mm}:{ss:02d}\"\n",
    "\n",
    "def print_prediction_with_loss(result):\n",
    "    total_scheduled = sum(result['work_sessions'])\n",
    "    count_S = result['pattern'].count('S')\n",
    "    count_L = result['pattern'].count('L')\n",
    "    short_break_total = (result['short_break'] or 0) * count_S\n",
    "    long_break_total = (result['long_break'] or 0) * count_L\n",
    "    total_scheduled += short_break_total + long_break_total\n",
    "    loss = abs(result['duration_minutes'] - total_scheduled)\n",
    "    print(result)\n",
    "    print(f\"Time loss: {loss:.2f} min ({format_minutes_to_mmss(loss)})\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b18f97cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'duration_minutes': 115, 'pattern': 'WSWSWL', 'work_sessions': [30, 30, 31], 'short_break': 5, 'long_break': 10}\n",
      "Time loss: 4.00 min (4:00)\n",
      "\n",
      "{'duration_minutes': 75, 'pattern': 'WSW', 'work_sessions': [27, 32], 'short_break': 5, 'long_break': None}\n",
      "Time loss: 11.00 min (11:00)\n",
      "\n",
      "{'duration_minutes': 200, 'pattern': 'WSWSWSWLWSWS', 'work_sessions': [31, 30, 29, 28, 29, 30], 'short_break': 5, 'long_break': 10}\n",
      "Time loss: 12.00 min (12:00)\n",
      "\n",
      "{'duration_minutes': 13.4, 'pattern': 'WSW', 'work_sessions': [7, 6], 'short_break': 0, 'long_break': None}\n",
      "Time loss: 0.40 min (0:24)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_prediction_with_loss(predict_full_delta(model, 115))\n",
    "print_prediction_with_loss(predict_full_delta(model, 75))\n",
    "print_prediction_with_loss(predict_full_delta(model, 200))\n",
    "print_prediction_with_loss(predict_full_delta(model, 13.4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
